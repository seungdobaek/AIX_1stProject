{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ í”„ë¡œì íŠ¸ - ë°ì´í„° ì „ì²˜ë¦¬ (ê°œì„  ë²„ì „)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =============================================================================\n",
    "# 1ë‹¨ê³„: ë²•ì •ë™ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"1ë‹¨ê³„: ë²•ì •ë™ ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "file_path = \"data/ë²•ì •ë™ì½”ë“œ ì¡°íšŒìë£Œ.xls\"\n",
    "dong_df = pd.read_excel(file_path)\n",
    "print(f\"âœ“ ë²•ì •ë™ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {dong_df.shape}\")\n",
    "\n",
    "# ì „ì²´ì½”ë“œ ë¶„ë¦¬ (ì• 5ìë¦¬/ë’¤ 5ìë¦¬)\n",
    "dong_df[\"ë²•ì •ë™ì½”ë“œ\"] = dong_df[\"ë²•ì •ë™ì½”ë“œ\"].astype(str).str.zfill(10)\n",
    "dong_df[\"ì‹œêµ°êµ¬ì½”ë“œ\"] = dong_df[\"ë²•ì •ë™ì½”ë“œ\"].str[:5]\n",
    "dong_df[\"ë²•ì •ë™ì½”ë“œ_í•˜ìœ„\"] = dong_df[\"ë²•ì •ë™ì½”ë“œ\"].str[5:]\n",
    "\n",
    "# ìƒìœ„ í–‰ì •êµ¬ì—­ ì œê±°\n",
    "before = len(dong_df)\n",
    "dong_df = dong_df[dong_df[\"ë²•ì •ë™ì½”ë“œ_í•˜ìœ„\"] != \"00000\"].copy()\n",
    "print(f\"âœ“ ìƒìœ„ í–‰ì •êµ¬ì—­ ì œê±°: {before} â†’ {len(dong_df)}\")\n",
    "\n",
    "# ë²•ì •ë™ ì „ì²´ëª… ë¶„ë¦¬\n",
    "dong_df[\"ë²•ì •ë™ëª…\"] = dong_df[\"ë²•ì •ë™ëª…\"].astype(str).str.strip()\n",
    "dong_df[\"ì‹œêµ°êµ¬ëª…\"] = dong_df[\"ë²•ì •ë™ëª…\"].apply(lambda x: \" \".join(x.split()[:-1]))\n",
    "dong_df[\"ë²•ì •ë™ëª…_í•˜ìœ„\"] = dong_df[\"ë²•ì •ë™ëª…\"].apply(lambda x: x.split()[-1])\n",
    "\n",
    "dong_clean = dong_df[[\n",
    "    \"ë²•ì •ë™ì½”ë“œ\", \"ì‹œêµ°êµ¬ì½”ë“œ\", \"ë²•ì •ë™ì½”ë“œ_í•˜ìœ„\", \n",
    "    \"ì‹œêµ°êµ¬ëª…\", \"ë²•ì •ë™ëª…_í•˜ìœ„\"\n",
    "]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "dong_clean.to_csv(\"data/ë²•ì •ë™_ë§ˆìŠ¤í„°.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ“ ë²•ì •ë™ ë§ˆìŠ¤í„° ìƒì„±: {dong_clean.shape}\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2ë‹¨ê³„: ì „ë ¥ëŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"2ë‹¨ê³„: ì „ë ¥ëŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ (ê°œì„  ë²„ì „)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 2-1. ì „ë ¥ëŸ‰ ë°ì´í„° ë¡œë“œ\n",
    "power_path = \"data/ë²•ì •ë™ë³„ì‹œê°„ë³„ì „ë ¥ì‚¬ìš©ëŸ‰.csv\"\n",
    "try:\n",
    "    power_df = pd.read_csv(power_path, encoding=\"utf-8-sig\")\n",
    "except UnicodeDecodeError:\n",
    "    power_df = pd.read_csv(power_path, encoding=\"cp949\")\n",
    "print(f\"âœ“ ì „ë ¥ëŸ‰ ë°ì´í„° ë¡œë“œ: {power_df.shape}\")\n",
    "\n",
    "# 2-2. ì¤‘ë³µ ì œê±°\n",
    "before = len(power_df)\n",
    "power_df = power_df.drop_duplicates().copy()\n",
    "print(f\"âœ“ ì¤‘ë³µ ì œê±°: {before} â†’ {len(power_df)}\")\n",
    "\n",
    "# 2-3. ë²•ì •ë™ì½”ë“œ ìƒì„± ë° ì¼ì‹œ ë³€í™˜\n",
    "power_df[\"SIGUNGU_CD\"] = power_df[\"SIGUNGU_CD\"].astype(str).str.zfill(5)\n",
    "power_df[\"BJDONG_CD\"] = power_df[\"BJDONG_CD\"].astype(str).str.zfill(5)\n",
    "power_df[\"ë²•ì •ë™ì½”ë“œ\"] = power_df[\"SIGUNGU_CD\"] + power_df[\"BJDONG_CD\"]\n",
    "\n",
    "power_df[\"USE_YM\"] = power_df[\"USE_YM\"].astype(str)\n",
    "power_df[\"USE_HM\"] = power_df[\"USE_HM\"].astype(str).str.zfill(4)\n",
    "power_df[\"ì¼ì‹œ\"] = pd.to_datetime(\n",
    "    power_df[\"USE_YM\"].str[:8] + power_df[\"USE_HM\"],\n",
    "    format=\"%Y%m%d%H%M\", \n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ì „ë ¥ëŸ‰ ìˆ«ì ë³€í™˜\n",
    "power_df[\"FDRCT_VLD_KWH\"] = pd.to_numeric(power_df[\"FDRCT_VLD_KWH\"], errors=\"coerce\")\n",
    "\n",
    "# 2-4. ì‹œê°„ ì •í•©ì„± ê²€ì¦ (ì¤‘ë³µ ì²˜ë¦¬)\n",
    "print(\"\\n[ì‹œê°„ ì •í•©ì„± ê²€ì¦]\")\n",
    "time_check = power_df.groupby([\"ë²•ì •ë™ì½”ë“œ\", \"ì¼ì‹œ\"]).size()\n",
    "duplicates = time_check[time_check > 1]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"âš ï¸ ì¤‘ë³µ ì‹œê°„ ë°œê²¬: {len(duplicates)}ê±´ â†’ í‰ê· ê°’ìœ¼ë¡œ ì§‘ê³„\")\n",
    "    power_df = power_df.groupby([\"ë²•ì •ë™ì½”ë“œ\", \"ì¼ì‹œ\"], as_index=False)[\"FDRCT_VLD_KWH\"].mean()\n",
    "else:\n",
    "    print(\"âœ“ ì¤‘ë³µ ì‹œê°„ ì—†ìŒ\")\n",
    "\n",
    "# 2-5. ì‹œê°„ë³„ ë°ì´í„° êµ¬ì¡°í™”\n",
    "power_hourly = power_df.copy()\n",
    "power_hourly[\"ì¼ì\"] = power_hourly[\"ì¼ì‹œ\"].dt.date\n",
    "power_hourly[\"ì‹œê°„\"] = power_hourly[\"ì¼ì‹œ\"].dt.hour\n",
    "power_hourly[\"ë…„\"] = power_hourly[\"ì¼ì‹œ\"].dt.year\n",
    "power_hourly[\"ì›”\"] = power_hourly[\"ì¼ì‹œ\"].dt.month\n",
    "\n",
    "print(f\"âœ“ ì‹œê°„ë³„ ë°ì´í„° ìƒì„±: {power_hourly.shape}\")\n",
    "\n",
    "# 2-6. ğŸ”¥ ì´ìƒì¹˜ ì™„í™” - Winsorizing (ë²¡í„°í™” ë²„ì „)\n",
    "print(\"\\n[ì „ë ¥ëŸ‰ ì´ìƒì¹˜ ì™„í™” - Winsorizing]\")\n",
    "print(\"ğŸ“ Note: Winsorizing ê¸°ì¤€(quantile)ì€ Train ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œë§Œ ì‚°ì¶œí•´ì•¼ í•¨\")\n",
    "\n",
    "# ë²•ì •ë™ë³„ 0.5%, 99.5% ë¶„ìœ„ìˆ˜ ê³„ì‚°\n",
    "q_values = power_hourly.groupby(\"ë²•ì •ë™ì½”ë“œ\")[\"FDRCT_VLD_KWH\"].quantile([0.005, 0.995]).unstack()\n",
    "q_values.columns = [\"lower_bound\", \"upper_bound\"]\n",
    "\n",
    "# ë³‘í•© í›„ clip\n",
    "power_hourly = power_hourly.merge(q_values, on=\"ë²•ì •ë™ì½”ë“œ\", how=\"left\")\n",
    "\n",
    "power_hourly[\"ì „ë ¥ëŸ‰_ì›ë³¸\"] = power_hourly[\"FDRCT_VLD_KWH\"]\n",
    "power_hourly[\"ì „ë ¥ëŸ‰_ì™„í™”\"] = power_hourly[\"FDRCT_VLD_KWH\"].clip(\n",
    "    lower=power_hourly[\"lower_bound\"],\n",
    "    upper=power_hourly[\"upper_bound\"]\n",
    ")\n",
    "\n",
    "# ì„ì‹œ ì»¬ëŸ¼ ì‚­ì œ\n",
    "power_hourly.drop(columns=[\"lower_bound\", \"upper_bound\"], inplace=True)\n",
    "\n",
    "print(\"âœ“ Winsorizing ì™„ë£Œ (ë²•ì •ë™ë³„ 0.5~99.5% ê¸°ì¤€)\")\n",
    "print(f\"  - ì›ë³¸ vs ì™„í™” ì°¨ì´: {(power_hourly['ì „ë ¥ëŸ‰_ì›ë³¸'] != power_hourly['ì „ë ¥ëŸ‰_ì™„í™”']).sum():,}ê±´\")\n",
    "\n",
    "# 2-7. ğŸ”¥ ì €ì „ë ¥ í”Œë˜ê·¸ ìƒì„± (ë²¡í„°í™” ë²„ì „)\n",
    "print(\"\\n[ì €ì „ë ¥ í”Œë˜ê·¸ ìƒì„±]\")\n",
    "\n",
    "# ë²•ì •ë™ë³„ í•˜ìœ„ 2% ì„ê³„ê°’ ê³„ì‚°\n",
    "low_threshold = power_hourly.groupby(\"ë²•ì •ë™ì½”ë“œ\")[\"ì „ë ¥ëŸ‰_ì™„í™”\"].quantile(0.02)\n",
    "low_threshold.name = \"ì €ì „ë ¥_ì„ê³„ê°’\"\n",
    "\n",
    "# ë³‘í•© í›„ í”Œë˜ê·¸ ìƒì„±\n",
    "power_hourly = power_hourly.merge(low_threshold, on=\"ë²•ì •ë™ì½”ë“œ\", how=\"left\")\n",
    "power_hourly[\"ì €ì „ë ¥_í”Œë˜ê·¸\"] = (\n",
    "    power_hourly[\"ì „ë ¥ëŸ‰_ì™„í™”\"] < power_hourly[\"ì €ì „ë ¥_ì„ê³„ê°’\"]\n",
    ").astype(int)\n",
    "\n",
    "# ì„ì‹œ ì»¬ëŸ¼ ì‚­ì œ\n",
    "power_hourly.drop(columns=[\"ì €ì „ë ¥_ì„ê³„ê°’\"], inplace=True)\n",
    "\n",
    "print(f\"âœ“ ì €ì „ë ¥ í”Œë˜ê·¸ ìƒì„± ì™„ë£Œ (í•˜ìœ„ 2% ê¸°ì¤€)\")\n",
    "print(f\"  - ì €ì „ë ¥ ë°ì´í„°: {power_hourly['ì €ì „ë ¥_í”Œë˜ê·¸'].sum():,}ê±´ ({power_hourly['ì €ì „ë ¥_í”Œë˜ê·¸'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 2-8. ë²•ì •ë™ ë³‘í•©\n",
    "merged_power = power_hourly.merge(dong_clean, on=\"ë²•ì •ë™ì½”ë“œ\", how=\"left\")\n",
    "print(f\"âœ“ ë²•ì •ë™ ë§ˆìŠ¤í„°ì™€ ë³‘í•©: {merged_power.shape}\")\n",
    "\n",
    "merged_power.to_csv(\"data/ì „ë ¥_ì‹œê°„ë³„_ë³‘í•©.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ“ ì €ì¥: data/ì „ë ¥_ì‹œê°„ë³„_ë³‘í•©.csv\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3ë‹¨ê³„: ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"3ë‹¨ê³„: ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_dir = \"data/weather_data\"\n",
    "\n",
    "# 3-1. ì‹œê°„ë³„(HR) ë‚ ì”¨ ë°ì´í„° ë¡œë“œ\n",
    "hr_files = [\n",
    "    \"SURFACE_ASOS_108_HR_2022_2022_2023.csv\",\n",
    "    \"SURFACE_ASOS_108_HR_2023_2023_2024.csv\",\n",
    "    \"SURFACE_ASOS_108_HR_2024_2024_2025.csv\"\n",
    "]\n",
    "\n",
    "hr_dfs = []\n",
    "for f in hr_files:\n",
    "    fpath = os.path.join(base_dir, f)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {f}\")\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(fpath, encoding=\"utf-8-sig\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(fpath, encoding=\"cp949\")\n",
    "    hr_dfs.append(df)\n",
    "\n",
    "if not hr_dfs:\n",
    "    raise FileNotFoundError(\"ì‹œê°„ë³„ ë‚ ì”¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "weather_hr = pd.concat(hr_dfs, ignore_index=True)\n",
    "print(f\"âœ“ ì‹œê°„ë³„ ë‚ ì”¨ ë°ì´í„° ë¡œë“œ: {weather_hr.shape}\")\n",
    "\n",
    "# 3-2. ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ íƒì§€\n",
    "date_col = None\n",
    "for c in weather_hr.columns:\n",
    "    if any(k in str(c).lower() for k in [\"tm\", \"ì¼ì‹œ\", \"time\"]):\n",
    "        s = pd.to_datetime(weather_hr[c], errors=\"coerce\")\n",
    "        if s.notna().sum() > len(weather_hr) * 0.9:\n",
    "            date_col = c\n",
    "            weather_hr[\"ì¼ì‹œ\"] = s\n",
    "            break\n",
    "\n",
    "if date_col is None:\n",
    "    raise ValueError(\"ë‚ ì”¨ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "weather_hr[\"ì¼ì\"] = weather_hr[\"ì¼ì‹œ\"].dt.date\n",
    "weather_hr[\"ì‹œê°„\"] = weather_hr[\"ì¼ì‹œ\"].dt.hour\n",
    "print(f\"âœ“ ë‚ ì§œ/ì‹œê°„ ë³€í™˜ ì™„ë£Œ: {date_col} â†’ ì¼ì‹œ\")\n",
    "\n",
    "# 3-3. í•„ìš”í•œ ì»¬ëŸ¼ ë§¤í•‘\n",
    "col_mapping = {\n",
    "    \"ê¸°ì˜¨(Â°C)\": \"ê¸°ì˜¨\",\n",
    "    \"ê°•ìˆ˜ëŸ‰(mm)\": \"ê°•ìˆ˜ëŸ‰\", \n",
    "    \"í’ì†(m/s)\": \"í’ì†\",\n",
    "    \"ìŠµë„(%)\": \"ìŠµë„\"\n",
    "}\n",
    "\n",
    "use_cols = [\"ì¼ì‹œ\", \"ì¼ì\", \"ì‹œê°„\"]\n",
    "for orig, new in col_mapping.items():\n",
    "    if orig in weather_hr.columns:\n",
    "        weather_hr[new] = pd.to_numeric(weather_hr[orig], errors=\"coerce\")\n",
    "        use_cols.append(new)\n",
    "\n",
    "weather_clean = weather_hr[use_cols].copy()\n",
    "print(f\"âœ“ ë‚ ì”¨ ì»¬ëŸ¼ ì„ íƒ: {[c for c in use_cols if c not in ['ì¼ì‹œ', 'ì¼ì', 'ì‹œê°„']]}\")\n",
    "\n",
    "# 3-4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "print(\"\\n[ê²°ì¸¡ì¹˜ ì²˜ë¦¬]\")\n",
    "if \"ê°•ìˆ˜ëŸ‰\" in weather_clean.columns:\n",
    "    weather_clean[\"ê°•ìˆ˜ëŸ‰\"] = weather_clean[\"ê°•ìˆ˜ëŸ‰\"].fillna(0)\n",
    "    print(\"âœ“ ê°•ìˆ˜ëŸ‰ ê²°ì¸¡ì¹˜ â†’ 0\")\n",
    "\n",
    "for col in [\"ê¸°ì˜¨\", \"í’ì†\", \"ìŠµë„\"]:\n",
    "    if col in weather_clean.columns:\n",
    "        before_na = weather_clean[col].isna().sum()\n",
    "        weather_clean[col] = weather_clean[col].interpolate(method=\"linear\")\n",
    "        weather_clean[col] = weather_clean[col].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        print(f\"âœ“ {col} ê²°ì¸¡ì¹˜ ë³´ê°„: {before_na} â†’ {weather_clean[col].isna().sum()}\")\n",
    "\n",
    "# 3-5. ì²´ê° ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜\n",
    "print(\"\\n[ê¸°ìƒ íŒŒìƒ ë³€ìˆ˜ ìƒì„±]\")\n",
    "\n",
    "if \"ê¸°ì˜¨\" in weather_clean.columns and \"ìŠµë„\" in weather_clean.columns:\n",
    "    T = weather_clean[\"ê¸°ì˜¨\"]\n",
    "    RH = weather_clean[\"ìŠµë„\"]\n",
    "    \n",
    "    # ë¶ˆì¾Œì§€ìˆ˜\n",
    "    weather_clean[\"ë¶ˆì¾Œì§€ìˆ˜\"] = 0.81 * T + 0.01 * RH * (0.99 * T - 14.3) + 46.3\n",
    "    print(\"âœ“ ë¶ˆì¾Œì§€ìˆ˜ ìƒì„±\")\n",
    "    \n",
    "    # ëƒ‰ë‚œë°©ë„ì¼ (Cooling/Heating Degree Days)\n",
    "    weather_clean[\"ëƒ‰ë°©ë„ì¼\"] = (weather_clean[\"ê¸°ì˜¨\"] - 24).clip(lower=0)\n",
    "    weather_clean[\"ë‚œë°©ë„ì¼\"] = (18 - weather_clean[\"ê¸°ì˜¨\"]).clip(lower=0)\n",
    "    print(\"âœ“ ëƒ‰ë‚œë°©ë„ì¼ ìƒì„± (ëƒ‰ë°©ê¸°ì¤€: 24Â°C, ë‚œë°©ê¸°ì¤€: 18Â°C)\")\n",
    "\n",
    "# 3-6. ê¸°ì˜¨ ë³€í™”ëŸ‰\n",
    "if \"ê¸°ì˜¨\" in weather_clean.columns:\n",
    "    weather_clean = weather_clean.sort_values(\"ì¼ì‹œ\").reset_index(drop=True)\n",
    "    weather_clean[\"ê¸°ì˜¨ë³€í™”_1h\"] = weather_clean[\"ê¸°ì˜¨\"].diff(1).fillna(0)\n",
    "    weather_clean[\"ê¸°ì˜¨ë³€í™”_24h\"] = weather_clean[\"ê¸°ì˜¨\"].diff(24).fillna(0)\n",
    "    print(\"âœ“ ê¸°ì˜¨ ë³€í™”ëŸ‰ ìƒì„± (1h, 24h)\")\n",
    "\n",
    "# 3-7. ê°•ìˆ˜ ì—¬ë¶€ í”Œë˜ê·¸\n",
    "if \"ê°•ìˆ˜ëŸ‰\" in weather_clean.columns:\n",
    "    weather_clean[\"ê°•ìˆ˜ì—¬ë¶€\"] = (weather_clean[\"ê°•ìˆ˜ëŸ‰\"] > 0).astype(int)\n",
    "    print(\"âœ“ ê°•ìˆ˜ ì—¬ë¶€ í”Œë˜ê·¸ ìƒì„±\")\n",
    "\n",
    "# ì‹œêµ°êµ¬ì½”ë“œ ë¶€ì—¬\n",
    "weather_clean[\"ì‹œêµ°êµ¬ì½”ë“œ\"] = \"11000\"\n",
    "\n",
    "weather_clean.to_csv(\"data/ë‚ ì”¨_ì‹œê°„ë³„_ì •ì œ.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ“ ì €ì¥: data/ë‚ ì”¨_ì‹œê°„ë³„_ì •ì œ.csv\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4ë‹¨ê³„: ìµœì¢… í†µí•©\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"4ë‹¨ê³„: ìµœì¢… ë°ì´í„° í†µí•©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4-1. ì „ë ¥ + ë‚ ì”¨ ë³‘í•©\n",
    "final_df = merged_power.merge(\n",
    "    weather_clean,\n",
    "    on=[\"ì¼ì\", \"ì‹œê°„\", \"ì‹œêµ°êµ¬ì½”ë“œ\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_ë‚ ì”¨\")\n",
    ")\n",
    "print(f\"âœ“ ì „ë ¥+ë‚ ì”¨ ë³‘í•© ì™„ë£Œ: {final_df.shape}\")\n",
    "\n",
    "# 4-2. ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ (ì´ë¯¸ ì „ë ¥ ë°ì´í„°ì— ì¡´ì¬)\n",
    "print(\"\\n[ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±]\")\n",
    "\n",
    "# ì¼ì‹œ í™•ì¸ ë° ë³µêµ¬\n",
    "if \"ì¼ì‹œ\" not in final_df.columns or final_df[\"ì¼ì‹œ\"].isna().all():\n",
    "    final_df[\"ì¼ì‹œ\"] = pd.to_datetime(final_df[\"ì¼ì\"]) + pd.to_timedelta(final_df[\"ì‹œê°„\"], unit='h')\n",
    "\n",
    "# ë…„ì›”ì¼ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)\n",
    "if \"ë…„\" not in final_df.columns:\n",
    "    final_df[\"ë…„\"] = final_df[\"ì¼ì‹œ\"].dt.year\n",
    "if \"ì›”\" not in final_df.columns:\n",
    "    final_df[\"ì›”\"] = final_df[\"ì¼ì‹œ\"].dt.month\n",
    "if \"ì¼\" not in final_df.columns:\n",
    "    final_df[\"ì¼\"] = final_df[\"ì¼ì‹œ\"].dt.day\n",
    "\n",
    "final_df[\"ìš”ì¼\"] = final_df[\"ì¼ì‹œ\"].dt.dayofweek  # 0=ì›”ìš”ì¼\n",
    "final_df[\"ì£¼ë§ì—¬ë¶€\"] = (final_df[\"ìš”ì¼\"] >= 5).astype(int)\n",
    "\n",
    "# 4-3. ê³µíœ´ì¼ í”Œë˜ê·¸\n",
    "holidays_2022_2025 = [\n",
    "    # 2022ë…„\n",
    "    \"2022-01-01\", \"2022-01-31\", \"2022-02-01\", \"2022-02-02\", \"2022-03-01\",\n",
    "    \"2022-03-09\", \"2022-05-05\", \"2022-05-08\", \"2022-06-06\", \"2022-08-15\",\n",
    "    \"2022-09-09\", \"2022-09-10\", \"2022-09-11\", \"2022-10-03\", \"2022-10-09\",\n",
    "    \"2022-10-10\", \"2022-12-25\",\n",
    "    # 2023ë…„\n",
    "    \"2023-01-01\", \"2023-01-21\", \"2023-01-22\", \"2023-01-23\", \"2023-01-24\",\n",
    "    \"2023-03-01\", \"2023-05-05\", \"2023-05-27\", \"2023-06-06\", \"2023-08-15\",\n",
    "    \"2023-09-28\", \"2023-09-29\", \"2023-09-30\", \"2023-10-03\", \"2023-10-09\",\n",
    "    \"2023-12-25\",\n",
    "    # 2024ë…„\n",
    "    \"2024-01-01\", \"2024-02-09\", \"2024-02-10\", \"2024-02-11\", \"2024-02-12\",\n",
    "    \"2024-03-01\", \"2024-04-10\", \"2024-05-05\", \"2024-05-06\", \"2024-05-15\",\n",
    "    \"2024-06-06\", \"2024-08-15\", \"2024-09-16\", \"2024-09-17\", \"2024-09-18\",\n",
    "    \"2024-10-03\", \"2024-10-09\", \"2024-12-25\",\n",
    "    # 2025ë…„\n",
    "    \"2025-01-01\", \"2025-01-28\", \"2025-01-29\", \"2025-01-30\", \"2025-03-01\",\n",
    "    \"2025-03-03\", \"2025-05-05\", \"2025-05-06\", \"2025-06-06\", \"2025-08-15\",\n",
    "    \"2025-10-03\", \"2025-10-05\", \"2025-10-06\", \"2025-10-07\", \"2025-10-08\",\n",
    "    \"2025-10-09\", \"2025-12-25\"\n",
    "]\n",
    "\n",
    "holidays_set = set(pd.to_datetime(holidays_2022_2025).date)\n",
    "final_df[\"ì¼ì_temp\"] = pd.to_datetime(final_df[\"ì¼ì\"]).dt.date\n",
    "final_df[\"ê³µíœ´ì¼ì—¬ë¶€\"] = final_df[\"ì¼ì_temp\"].isin(holidays_set).astype(int)\n",
    "final_df.drop(\"ì¼ì_temp\", axis=1, inplace=True)\n",
    "\n",
    "print(\"âœ“ ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ì™„ë£Œ (ë…„ì›”ì¼, ìš”ì¼, ì£¼ë§, ê³µíœ´ì¼)\")\n",
    "\n",
    "# 4-4. ë²•ì •ë™ë³„ í†µê³„ Feature (Train ê¸°ê°„ ê¸°ì¤€)\n",
    "print(\"\\n[ë²•ì •ë™ë³„ í†µê³„ Feature ìƒì„±]\")\n",
    "print(\"ğŸ“ Note: Train ê¸°ê°„(2022~2024) ê¸°ì¤€ìœ¼ë¡œë§Œ ê³„ì‚° (Data Leakage ë°©ì§€)\")\n",
    "\n",
    "train_mask = final_df[\"ë…„\"] <= 2024\n",
    "train_data = final_df[train_mask].copy()\n",
    "\n",
    "# ë²•ì •ë™ë³„ ê¸°ë³¸ í†µê³„\n",
    "dong_stats = train_data.groupby(\"ë²•ì •ë™ì½”ë“œ\")[\"ì „ë ¥ëŸ‰_ì™„í™”\"].agg([\"mean\", \"std\", \"max\"]).reset_index()\n",
    "dong_stats.columns = [\"ë²•ì •ë™ì½”ë“œ\", \"ë²•ì •ë™_í‰ê· ì „ë ¥\", \"ë²•ì •ë™_í‘œì¤€í¸ì°¨\", \"ë²•ì •ë™_ìµœëŒ€ì „ë ¥\"]\n",
    "\n",
    "# ë²•ì •ë™Ã—ì‹œê°„ëŒ€ë³„ í‰ê· \n",
    "dong_hour_stats = train_data.groupby([\"ë²•ì •ë™ì½”ë“œ\", \"ì‹œê°„\"])[\"ì „ë ¥ëŸ‰_ì™„í™”\"].mean().reset_index()\n",
    "dong_hour_stats.columns = [\"ë²•ì •ë™ì½”ë“œ\", \"ì‹œê°„\", \"ë²•ì •ë™_ì‹œê°„í‰ê· ì „ë ¥\"]\n",
    "\n",
    "# ë³‘í•©\n",
    "final_df = final_df.merge(dong_stats, on=\"ë²•ì •ë™ì½”ë“œ\", how=\"left\")\n",
    "final_df = final_df.merge(dong_hour_stats, on=[\"ë²•ì •ë™ì½”ë“œ\", \"ì‹œê°„\"], how=\"left\")\n",
    "\n",
    "print(\"âœ“ ë²•ì •ë™ í†µê³„ Feature ë³‘í•© ì™„ë£Œ\")\n",
    "print(f\"  - ë²•ì •ë™_í‰ê· ì „ë ¥, í‘œì¤€í¸ì°¨, ìµœëŒ€ì „ë ¥\")\n",
    "print(f\"  - ë²•ì •ë™_ì‹œê°„í‰ê· ì „ë ¥ (ì‹œê°„ëŒ€ë³„)\")\n",
    "\n",
    "# 4-5. ìµœì¢… ì €ì¥\n",
    "final_df.to_csv(\"data/ìµœì¢…_í†µí•©ë°ì´í„°_ì‹œê°„ë³„.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“Š ìµœì¢… ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"  - ì €ì¥ ê²½ë¡œ: data/ìµœì¢…_í†µí•©ë°ì´í„°_ì‹œê°„ë³„.csv\")\n",
    "print(f\"  - ì´ ë°ì´í„°: {len(final_df):,}ê°œ\")\n",
    "print(f\"  - ì»¬ëŸ¼ ìˆ˜: {len(final_df.columns)}ê°œ\")\n",
    "print(f\"  - ê¸°ê°„: {pd.to_datetime(final_df['ì¼ì']).min().date()} ~ {pd.to_datetime(final_df['ì¼ì']).max().date()}\")\n",
    "print(f\"  - ë²•ì •ë™ ìˆ˜: {final_df['ë²•ì •ë™ì½”ë“œ'].nunique()}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ì²´í¬\n",
    "print(\"\\nğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "null_summary = final_df.isnull().sum()\n",
    "if null_summary.sum() > 0:\n",
    "    print(\"  ê²°ì¸¡ì¹˜ í˜„í™© (ìƒìœ„ 5ê°œ):\")\n",
    "    for col, count in null_summary[null_summary > 0].head().items():\n",
    "        print(f\"    - {col}: {count:,}ê°œ ({count/len(final_df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"  âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ ì£¼ìš” ë³€ìˆ˜ í†µê³„:\")\n",
    "key_cols = [\"ì „ë ¥ëŸ‰_ì™„í™”\", \"ê¸°ì˜¨\", \"ë¶ˆì¾Œì§€ìˆ˜\"]\n",
    "for col in key_cols:\n",
    "    if col in final_df.columns:\n",
    "        print(f\"\\n  {col}:\")\n",
    "        print(f\"    í‰ê· : {final_df[col].mean():.2f}\")\n",
    "        print(f\"    í‘œì¤€í¸ì°¨: {final_df[col].std():.2f}\")\n",
    "        print(f\"    ìµœì†Œ: {final_df[col].min():.2f}\")\n",
    "        print(f\"    ìµœëŒ€: {final_df[col].max():.2f}\")\n",
    "\n",
    "print(\"\\nâœ… ì „ì²´ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
