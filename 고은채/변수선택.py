"""
ë³€ìˆ˜ ì„ íƒ ë° EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)
====================================

ëª©í‘œ: í›ˆë ¨ì— ì‚¬ìš©í•  ë³€ìˆ˜ 10ê°œ ì„ íƒ

ë°ì´í„°: ìµœì¢…_í†µí•©_ë°ì´í„°_ì™„ë²½.csv
ë¶„ì„ í•­ëª©:
1. ê¸°ë³¸ ë°ì´í„° íƒìƒ‰
2. ê²°ì¸¡ì¹˜ ë¶„ì„
3. ë¶„í¬ ë¶„ì„
4. ìƒê´€ê´€ê³„ ë¶„ì„
5. ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
6. ìµœì¢… ë³€ìˆ˜ 10ê°œ ì¶”ì²œ

í•„ìš”í•œ íŒ¨í‚¤ì§€:
pip install pandas numpy matplotlib seaborn scipy scikit-learn

ì‚¬ìš©ë²•:
python ë³€ìˆ˜ì„ íƒ_EDA.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import os
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100

# ============================================================================
# ë©”ì¸ í”„ë¡œê·¸ë¨
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("="*100)
    print("ë³€ìˆ˜ ì„ íƒ ë° EDA (Exploratory Data Analysis)")
    print("="*100)
    print("\nëª©í‘œ: í›ˆë ¨ì— ì‚¬ìš©í•  ë³€ìˆ˜ 10ê°œ ì„ íƒ")
    
    data_file = 'ìµœì¢…_í†µí•©_ë°ì´í„°_ì™„ë²½.csv'
    
    if not os.path.exists(data_file):
        print(f"\nâŒ '{data_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ“ ë°ì´í„° íŒŒì¼ ë°œê²¬: {data_file}")
    
    # ë©”ë‰´
    while True:
        print("\n" + "="*100)
        print("ë¶„ì„ ë‹¨ê³„ ì„ íƒ:")
        print("="*100)
        print("\n1. ğŸ“Š 1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° íƒìƒ‰")
        print("2. ğŸ” 2ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ë¶„ì„")
        print("3. ğŸ“ˆ 3ë‹¨ê³„: ë¶„í¬ ë¶„ì„")
        print("4. ğŸ”— 4ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„")
        print("5. ğŸ¯ 5ë‹¨ê³„: ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„")
        print("6. âœ… 6ë‹¨ê³„: ìµœì¢… ë³€ìˆ˜ 10ê°œ ì¶”ì²œ")
        print("7. ğŸ“‹ ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±")
        print("0. âŒ ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ: ").strip()
        
        if choice == '1':
            step1_basic_exploration(data_file)
        elif choice == '2':
            step2_missing_outliers(data_file)
        elif choice == '3':
            step3_distribution(data_file)
        elif choice == '4':
            step4_correlation(data_file)
        elif choice == '5':
            step5_feature_importance(data_file)
        elif choice == '6':
            step6_final_selection(data_file)
        elif choice == '7':
            generate_full_report(data_file)
        elif choice == '0':
            print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("\nâŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


# ============================================================================
# 1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° íƒìƒ‰
# ============================================================================

def step1_basic_exploration(data_file):
    """1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° íƒìƒ‰"""
    print("\n" + "="*100)
    print("ğŸ“Š 1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° íƒìƒ‰")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    print(f"\n[1] ë°ì´í„° ê¸°ë³¸ ì •ë³´")
    print(f"  - í–‰ ìˆ˜: {len(df):,}")
    print(f"  - ì—´ ìˆ˜: {len(df.columns)}")
    print(f"  - ê¸°ê°„: {df['ì¼ì'].min().date()} ~ {df['ì¼ì'].max().date()}")
    print(f"  - ì´ ì¼ìˆ˜: {(df['ì¼ì'].max() - df['ì¼ì'].min()).days + 1}ì¼")
    
    print(f"\n[2] ì»¬ëŸ¼ ëª©ë¡ ë° ë°ì´í„° íƒ€ì…")
    for i, col in enumerate(df.columns, 1):
        dtype = df[col].dtype
        non_null = df[col].notna().sum()
        null_pct = (df[col].isna().sum() / len(df)) * 100
        print(f"  {i:2d}. {col:30s} | {str(dtype):10s} | Non-null: {non_null:6,} ({100-null_pct:5.1f}%)")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    
    print(f"\n[3] íƒ€ê²Ÿ ë³€ìˆ˜: {target}")
    print(f"  - í‰ê· : {df[target].mean():,.0f} kWh")
    print(f"  - ì¤‘ì•™ê°’: {df[target].median():,.0f} kWh")
    print(f"  - í‘œì¤€í¸ì°¨: {df[target].std():,.0f} kWh")
    print(f"  - ìµœì†Œ: {df[target].min():,.0f} kWh")
    print(f"  - ìµœëŒ€: {df[target].max():,.0f} kWh")
    print(f"  - ë³€ë™ê³„ìˆ˜(CV): {(df[target].std() / df[target].mean()):.2f}")
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ êµ¬ë¶„
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    # ì œì™¸í•  ì»¬ëŸ¼
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_features = [col for col in numeric_cols if col not in exclude_cols and col != target]
    
    print(f"\n[4] ë³€ìˆ˜ íƒ€ì… ë¶„ë¥˜")
    print(f"  - ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_features)}ê°œ")
    print(f"  - ë²”ì£¼í˜• ë³€ìˆ˜: {len(categorical_cols)}ê°œ")
    
    print(f"\n[5] ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ëª©ë¡ ({len(numeric_features)}ê°œ)")
    for i, col in enumerate(numeric_features, 1):
        print(f"  {i:2d}. {col}")
    
    print(f"\n[6] ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡ ({len(categorical_cols)}ê°œ)")
    for i, col in enumerate(categorical_cols, 1):
        unique_count = df[col].nunique()
        print(f"  {i:2d}. {col:20s} - {unique_count}ê°œ ê³ ìœ ê°’")
    
    print("\nâœ“ 1ë‹¨ê³„ ì™„ë£Œ!")


# ============================================================================
# 2ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ë¶„ì„
# ============================================================================

def step2_missing_outliers(data_file):
    """2ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ë¶„ì„"""
    print("\n" + "="*100)
    print("ğŸ” 2ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ë¶„ì„")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    # ê²°ì¸¡ì¹˜ ë¶„ì„
    print("\n[1] ê²°ì¸¡ì¹˜ ë¶„ì„")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    missing_df = pd.DataFrame({
        'ì»¬ëŸ¼ëª…': missing.index,
        'ê²°ì¸¡ì¹˜ìˆ˜': missing.values,
        'ë¹„ìœ¨(%)': missing_pct.values
    })
    missing_df = missing_df[missing_df['ê²°ì¸¡ì¹˜ìˆ˜'] > 0].sort_values('ê²°ì¸¡ì¹˜ìˆ˜', ascending=False)
    
    if len(missing_df) > 0:
        print(missing_df.to_string(index=False))
    else:
        print("  âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ!")
    
    # ì´ìƒì¹˜ ë¶„ì„ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜)
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [col for col in numeric_cols if col not in exclude_cols]
    
    print(f"\n[2] ì´ìƒì¹˜ ë¶„ì„ (IQR ë°©ë²•)")
    print(f"  ê¸°ì¤€: Q1 - 1.5*IQR ë¯¸ë§Œ ë˜ëŠ” Q3 + 1.5*IQR ì´ˆê³¼")
    
    outlier_summary = []
    for col in numeric_features:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outlier_count = len(outliers)
        outlier_pct = (outlier_count / len(df)) * 100
        
        outlier_summary.append({
            'ë³€ìˆ˜ëª…': col,
            'ì´ìƒì¹˜ìˆ˜': outlier_count,
            'ë¹„ìœ¨(%)': outlier_pct,
            'í•˜í•œ': lower_bound,
            'ìƒí•œ': upper_bound
        })
    
    outlier_df = pd.DataFrame(outlier_summary)
    outlier_df = outlier_df[outlier_df['ì´ìƒì¹˜ìˆ˜'] > 0].sort_values('ì´ìƒì¹˜ìˆ˜', ascending=False)
    
    if len(outlier_df) > 0:
        print("\n  ì´ìƒì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜:")
        for _, row in outlier_df.head(10).iterrows():
            print(f"    {row['ë³€ìˆ˜ëª…']:30s}: {row['ì´ìƒì¹˜ìˆ˜']:5d}ê°œ ({row['ë¹„ìœ¨(%)']:5.1f}%)")
    else:
        print("  âœ“ ì´ìƒì¹˜ ì—†ìŒ!")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„
    print(f"\n[3] íƒ€ê²Ÿ ë³€ìˆ˜ ì´ìƒì¹˜ ìƒì„¸")
    Q1 = df[target].quantile(0.25)
    Q3 = df[target].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    
    outliers = df[(df[target] < lower) | (df[target] > upper)]
    print(f"  - ì´ìƒì¹˜ ê°œìˆ˜: {len(outliers):,}ê°œ ({len(outliers)/len(df)*100:.1f}%)")
    print(f"  - í•˜í•œ: {lower:,.0f} kWh")
    print(f"  - ìƒí•œ: {upper:,.0f} kWh")
    
    if len(outliers) > 0:
        print(f"\n  ìƒìœ„ 5ê°œ ì´ìƒì¹˜:")
        top_outliers = df.nlargest(5, target)[['ì¼ì', 'ë²•ì •ë™ëª…', target, 'í‰ê· ê¸°ì˜¨(Â°C)']]
        print(top_outliers.to_string(index=False))
    
    print("\nâœ“ 2ë‹¨ê³„ ì™„ë£Œ!")


# ============================================================================
# 3ë‹¨ê³„: ë¶„í¬ ë¶„ì„
# ============================================================================

def step3_distribution(data_file):
    """3ë‹¨ê³„: ë¶„í¬ ë¶„ì„"""
    print("\n" + "="*100)
    print("ğŸ“ˆ 3ë‹¨ê³„: ë¶„í¬ ë¶„ì„")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [col for col in numeric_cols if col not in exclude_cols and col != target]
    
    # ì •ê·œì„± ê²€ì •
    print("\n[1] ì •ê·œì„± ê²€ì • (Shapiro-Wilk Test)")
    print("  ê¸°ì¤€: p-value > 0.05 â†’ ì •ê·œë¶„í¬")
    
    # ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ)
    sample_size = min(5000, len(df))
    df_sample = df.sample(n=sample_size, random_state=42)
    
    normality_results = []
    for col in numeric_features:
        if df_sample[col].notna().sum() > 3:  # ìµœì†Œ 3ê°œ ì´ìƒ
            stat, p_value = stats.shapiro(df_sample[col].dropna())
            is_normal = "ì •ê·œë¶„í¬" if p_value > 0.05 else "ë¹„ì •ê·œ"
            normality_results.append({
                'ë³€ìˆ˜ëª…': col,
                'p-value': p_value,
                'íŒì •': is_normal
            })
    
    norm_df = pd.DataFrame(normality_results).sort_values('p-value', ascending=False)
    print("\n  ì •ê·œì„± ê²€ì • ê²°ê³¼ (ìƒìœ„ 15ê°œ):")
    for _, row in norm_df.head(15).iterrows():
        print(f"    {row['ë³€ìˆ˜ëª…']:30s}: p={row['p-value']:.4f} ({row['íŒì •']})")
    
    # ì™œë„ ë° ì²¨ë„
    print("\n[2] ì™œë„(Skewness) ë° ì²¨ë„(Kurtosis)")
    print("  ì™œë„: 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëŒ€ì¹­, |ì™œë„| < 1 ì ì ˆ")
    print("  ì²¨ë„: 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ê·œë¶„í¬, |ì²¨ë„| < 3 ì ì ˆ")
    
    skew_kurt = []
    for col in numeric_features:
        skewness = df[col].skew()
        kurtosis = df[col].kurtosis()
        skew_kurt.append({
            'ë³€ìˆ˜ëª…': col,
            'ì™œë„': skewness,
            'ì²¨ë„': kurtosis,
            'ì™œë„íŒì •': 'ì ì ˆ' if abs(skewness) < 1 else 'ì¹˜ìš°ì¹¨',
            'ì²¨ë„íŒì •': 'ì ì ˆ' if abs(kurtosis) < 3 else 'ë¾°ì¡±/í‰í‰'
        })
    
    sk_df = pd.DataFrame(skew_kurt)
    print("\n  ì™œë„ê°€ ì ì ˆí•œ ë³€ìˆ˜:")
    appropriate = sk_df[sk_df['ì™œë„íŒì •'] == 'ì ì ˆ'].sort_values('ì™œë„', key=abs)
    for _, row in appropriate.head(10).iterrows():
        print(f"    {row['ë³€ìˆ˜ëª…']:30s}: ì™œë„={row['ì™œë„']:6.2f}, ì²¨ë„={row['ì²¨ë„']:6.2f}")
    
    # ì‹œê°í™”
    print("\n[3] ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")
    output_dir = 'eda_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ì£¼ìš” ë³€ìˆ˜ 4ê°œ ë¶„í¬
    key_vars = ['í‰ê· ê¸°ì˜¨(Â°C)', 'ìµœì €ê¸°ì˜¨(Â°C)', 'ìµœê³ ê¸°ì˜¨(Â°C)', 'ì¼ê°•ìˆ˜ëŸ‰(mm)']
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    for i, var in enumerate(key_vars):
        axes[i].hist(df[var], bins=50, alpha=0.7, edgecolor='black')
        axes[i].set_xlabel(var, fontsize=10)
        axes[i].set_ylabel('Frequency', fontsize=10)
        axes[i].set_title(f'Distribution of {var}', fontsize=11, fontweight='bold')
        axes[i].grid(True, alpha=0.3)
        
        # í†µê³„ëŸ‰ í‘œì‹œ
        mean = df[var].mean()
        median = df[var].median()
        axes[i].axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.1f}')
        axes[i].axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:.1f}')
        axes[i].legend()
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/3_distribution_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ì €ì¥: {output_dir}/3_distribution_analysis.png")
    
    print("\nâœ“ 3ë‹¨ê³„ ì™„ë£Œ!")


# ============================================================================
# 4ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„
# ============================================================================

def step4_correlation(data_file):
    """4ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„"""
    print("\n" + "="*100)
    print("ğŸ”— 4ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [col for col in numeric_cols if col not in exclude_cols]
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    corr_matrix = df[numeric_features].corr()
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„
    print("\n[1] íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ (Pearson)")
    target_corr = corr_matrix[target].drop(target).sort_values(ascending=False, key=abs)
    
    print("\n  ìƒê´€ê³„ìˆ˜ê°€ ë†’ì€ ë³€ìˆ˜ (Top 15):")
    for var, corr in target_corr.head(15).items():
        strength = "ë§¤ìš°ê°•í•¨" if abs(corr) >= 0.7 else "ê°•í•¨" if abs(corr) >= 0.5 else "ì¤‘ê°„" if abs(corr) >= 0.3 else "ì•½í•¨"
        print(f"    {var:30s}: {corr:7.4f} ({strength})")
    
    print("\n  ìƒê´€ê³„ìˆ˜ê°€ ë‚®ì€ ë³€ìˆ˜ (Bottom 5):")
    for var, corr in target_corr.tail(5).items():
        print(f"    {var:30s}: {corr:7.4f}")
    
    # ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬
    print("\n[2] ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬ (ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„)")
    print("  ê¸°ì¤€: |ìƒê´€ê³„ìˆ˜| > 0.8 â†’ ë‹¤ì¤‘ê³µì„ ì„± ì˜ì‹¬")
    
    high_corr_pairs = []
    for i in range(len(numeric_features)):
        for j in range(i+1, len(numeric_features)):
            var1 = numeric_features[i]
            var2 = numeric_features[j]
            if var1 in corr_matrix.columns and var2 in corr_matrix.columns:
                corr_val = corr_matrix.loc[var1, var2]
                if abs(corr_val) > 0.8:
                    high_corr_pairs.append({
                        'ë³€ìˆ˜1': var1,
                        'ë³€ìˆ˜2': var2,
                        'ìƒê´€ê³„ìˆ˜': corr_val
                    })
    
    if high_corr_pairs:
        print(f"\n  ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒ ({len(high_corr_pairs)}ê°œ):")
        for pair in sorted(high_corr_pairs, key=lambda x: abs(x['ìƒê´€ê³„ìˆ˜']), reverse=True)[:10]:
            print(f"    {pair['ë³€ìˆ˜1']:25s} - {pair['ë³€ìˆ˜2']:25s}: {pair['ìƒê´€ê³„ìˆ˜']:6.3f}")
    else:
        print("  âœ“ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì—†ìŒ!")
    
    # ì‹œê°í™”
    print("\n[3] ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
    output_dir = 'eda_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ì£¼ìš” ë³€ìˆ˜ë§Œ ì„ íƒ (ìƒìœ„ 12ê°œ)
    top_vars = target_corr.head(12).index.tolist()
    top_vars.insert(0, target)
    
    plt.figure(figsize=(14, 12))
    sns.heatmap(df[top_vars].corr(), annot=True, fmt='.2f', cmap='RdBu_r', 
                center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title('Correlation Heatmap (Top 12 Variables + Target)', 
              fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/4_correlation_heatmap.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ì €ì¥: {output_dir}/4_correlation_heatmap.png")
    
    # íƒ€ê²Ÿê³¼ì˜ ì‚°ì ë„
    fig, axes = plt.subplots(3, 3, figsize=(15, 15))
    axes = axes.ravel()
    
    top_9_vars = target_corr.head(9).index.tolist()
    
    for i, var in enumerate(top_9_vars):
        axes[i].scatter(df[var], df[target]/1000000, alpha=0.3, s=5)
        axes[i].set_xlabel(var, fontsize=9)
        axes[i].set_ylabel('Power (GWh)', fontsize=9)
        corr = target_corr[var]
        axes[i].set_title(f'{var}\n(r = {corr:.3f})', fontsize=10, fontweight='bold')
        axes[i].grid(True, alpha=0.3)
    
    plt.suptitle('Scatter Plots: Top 9 Correlated Variables vs Target', 
                 fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/4_scatter_top9.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ì €ì¥: {output_dir}/4_scatter_top9.png")
    
    print("\nâœ“ 4ë‹¨ê³„ ì™„ë£Œ!")


# ============================================================================
# 5ë‹¨ê³„: ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
# ============================================================================

def step5_feature_importance(data_file):
    """5ë‹¨ê³„: ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„"""
    print("\n" + "="*100)
    print("ğŸ¯ 5ë‹¨ê³„: ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„ (Random Forest)")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [col for col in numeric_cols if col not in exclude_cols and col != target]
    
    print("\n[1] Random Forest ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    df_clean = df[numeric_features + [target]].dropna()
    
    X = df_clean[numeric_features]
    y = df_clean[target]
    
    # ëª¨ë¸ í•™ìŠµ
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)
    rf.fit(X, y)
    
    # ë³€ìˆ˜ ì¤‘ìš”ë„
    importances = rf.feature_importances_
    feature_importance = pd.DataFrame({
        'ë³€ìˆ˜ëª…': numeric_features,
        'ì¤‘ìš”ë„': importances
    }).sort_values('ì¤‘ìš”ë„', ascending=False)
    
    print("\n[2] ë³€ìˆ˜ ì¤‘ìš”ë„ ìˆœìœ„ (Top 20)")
    for i, row in feature_importance.head(20).iterrows():
        print(f"  {i+1:2d}. {row['ë³€ìˆ˜ëª…']:30s}: {row['ì¤‘ìš”ë„']:.4f}")
    
    # ì‹œê°í™”
    print("\n[3] ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™” ìƒì„± ì¤‘...")
    output_dir = 'eda_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    plt.figure(figsize=(12, 10))
    top_20 = feature_importance.head(20)
    colors_grad = plt.cm.viridis(np.linspace(0, 1, 20))
    
    plt.barh(range(len(top_20)), top_20['ì¤‘ìš”ë„'].values, color=colors_grad)
    plt.yticks(range(len(top_20)), top_20['ë³€ìˆ˜ëª…'].values)
    plt.xlabel('Feature Importance', fontsize=12)
    plt.title('Top 20 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='x')
    
    # ê°’ í‘œì‹œ
    for i, (idx, row) in enumerate(top_20.iterrows()):
        plt.text(row['ì¤‘ìš”ë„'], i, f" {row['ì¤‘ìš”ë„']:.4f}", 
                 va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/5_feature_importance.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ì €ì¥: {output_dir}/5_feature_importance.png")
    
    print("\nâœ“ 5ë‹¨ê³„ ì™„ë£Œ!")
    
    return feature_importance


# ============================================================================
# 6ë‹¨ê³„: ìµœì¢… ë³€ìˆ˜ 10ê°œ ì¶”ì²œ
# ============================================================================

def step6_final_selection(data_file):
    """6ë‹¨ê³„: ìµœì¢… ë³€ìˆ˜ 10ê°œ ì¶”ì²œ"""
    print("\n" + "="*100)
    print("âœ… 6ë‹¨ê³„: ìµœì¢… ë³€ìˆ˜ 10ê°œ ì„ íƒ")
    print("="*100)
    
    df = pd.read_csv(data_file)
    df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
    
    target = 'ì¼ë³„_ì „ë ¥ì‚¬ìš©ëŸ‰_KWH'
    exclude_cols = ['BJDONG_CD', 'ë²•ì •ë™ì½”ë“œ', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ë²•ì •ë™_ì„¸ë¶€ì½”ë“œ']
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [col for col in numeric_cols if col not in exclude_cols and col != target]
    
    # 1. ìƒê´€ê³„ìˆ˜
    print("\n[1] ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ì¶”ì²œ")
    corr_matrix = df[numeric_features + [target]].corr()
    target_corr = corr_matrix[target].drop(target).sort_values(ascending=False, key=abs)
    corr_top10 = target_corr.head(10)
    
    print("  ìƒê´€ê³„ìˆ˜ Top 10:")
    for i, (var, corr) in enumerate(corr_top10.items(), 1):
        print(f"    {i:2d}. {var:30s}: {corr:7.4f}")
    
    # 2. Random Forest ì¤‘ìš”ë„
    print("\n[2] Random Forest ì¤‘ìš”ë„ ê¸°ë°˜ ì¶”ì²œ")
    df_clean = df[numeric_features + [target]].dropna()
    X = df_clean[numeric_features]
    y = df_clean[target]
    
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)
    rf.fit(X, y)
    
    importances = pd.DataFrame({
        'ë³€ìˆ˜ëª…': numeric_features,
        'ì¤‘ìš”ë„': rf.feature_importances_
    }).sort_values('ì¤‘ìš”ë„', ascending=False)
    
    rf_top10 = importances.head(10)
    
    print("  ì¤‘ìš”ë„ Top 10:")
    for i, row in rf_top10.iterrows():
        print(f"    {i+1:2d}. {row['ë³€ìˆ˜ëª…']:30s}: {row['ì¤‘ìš”ë„']:.4f}")
    
    # 3. ì¢…í•© ì ìˆ˜ (ìƒê´€ê³„ìˆ˜ + ì¤‘ìš”ë„)
    print("\n[3] ì¢…í•© ì ìˆ˜ ê¸°ë°˜ ìµœì¢… ì¶”ì²œ")
    
    # ì •ê·œí™”
    corr_norm = (abs(target_corr) - abs(target_corr).min()) / (abs(target_corr).max() - abs(target_corr).min())
    imp_norm = (importances.set_index('ë³€ìˆ˜ëª…')['ì¤‘ìš”ë„'] - importances['ì¤‘ìš”ë„'].min()) / \
               (importances['ì¤‘ìš”ë„'].max() - importances['ì¤‘ìš”ë„'].min())
    
    # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘í‰ê· : ìƒê´€ê³„ìˆ˜ 40%, ì¤‘ìš”ë„ 60%)
    combined_score = {}
    for var in numeric_features:
        corr_score = corr_norm.get(var, 0) * 0.4
        imp_score = imp_norm.get(var, 0) * 0.6
        combined_score[var] = corr_score + imp_score
    
    combined_df = pd.DataFrame(list(combined_score.items()), 
                                columns=['ë³€ìˆ˜ëª…', 'ì¢…í•©ì ìˆ˜']).sort_values('ì¢…í•©ì ìˆ˜', ascending=False)
    
    final_top10 = combined_df.head(10)
    
    print("\n  â˜… ìµœì¢… ì¶”ì²œ ë³€ìˆ˜ 10ê°œ â˜…")
    for i, row in final_top10.iterrows():
        var = row['ë³€ìˆ˜ëª…']
        score = row['ì¢…í•©ì ìˆ˜']
        corr = target_corr.get(var, 0)
        imp = importances[importances['ë³€ìˆ˜ëª…'] == var]['ì¤‘ìš”ë„'].values[0] if var in importances['ë³€ìˆ˜ëª…'].values else 0
        
        print(f"    {i+1:2d}. {var:30s}")
        print(f"        - ì¢…í•©ì ìˆ˜: {score:.4f}")
        print(f"        - ìƒê´€ê³„ìˆ˜: {corr:7.4f}")
        print(f"        - ì¤‘ìš”ë„:   {imp:.4f}")
    
    # ì €ì¥
    output_dir = 'eda_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # CSV ì €ì¥
    final_selection = pd.DataFrame({
        'ìˆœìœ„': range(1, 11),
        'ë³€ìˆ˜ëª…': final_top10['ë³€ìˆ˜ëª…'].values,
        'ì¢…í•©ì ìˆ˜': final_top10['ì¢…í•©ì ìˆ˜'].values,
        'ìƒê´€ê³„ìˆ˜': [target_corr.get(var, 0) for var in final_top10['ë³€ìˆ˜ëª…']],
        'ì¤‘ìš”ë„': [importances[importances['ë³€ìˆ˜ëª…'] == var]['ì¤‘ìš”ë„'].values[0] 
                 if var in importances['ë³€ìˆ˜ëª…'].values else 0 
                 for var in final_top10['ë³€ìˆ˜ëª…']]
    })
    
    final_selection.to_csv(f'{output_dir}/ìµœì¢…ì„ íƒ_ë³€ìˆ˜10ê°œ.csv', index=False, encoding='utf-8-sig')
    print(f"\n  âœ“ ì €ì¥: {output_dir}/ìµœì¢…ì„ íƒ_ë³€ìˆ˜10ê°œ.csv")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ìƒê´€ê³„ìˆ˜ vs ì¤‘ìš”ë„
    axes[0].scatter([corr_norm.get(var, 0) for var in final_top10['ë³€ìˆ˜ëª…']],
                    [imp_norm.get(var, 0) for var in final_top10['ë³€ìˆ˜ëª…']],
                    s=200, alpha=0.6, c=range(10), cmap='viridis')
    
    for i, var in enumerate(final_top10['ë³€ìˆ˜ëª…']):
        axes[0].annotate(f"{i+1}", 
                        (corr_norm.get(var, 0), imp_norm.get(var, 0)),
                        ha='center', va='center', fontweight='bold')
    
    axes[0].set_xlabel('Normalized Correlation (40%)', fontsize=11)
    axes[0].set_ylabel('Normalized Importance (60%)', fontsize=11)
    axes[0].set_title('Feature Selection: Correlation vs Importance', 
                     fontsize=12, fontweight='bold')
    axes[0].grid(True, alpha=0.3)
    
    # ì¢…í•© ì ìˆ˜ ë°” ì°¨íŠ¸
    colors = plt.cm.viridis(np.linspace(0, 1, 10))
    axes[1].barh(range(10), final_top10['ì¢…í•©ì ìˆ˜'].values, color=colors)
    axes[1].set_yticks(range(10))
    axes[1].set_yticklabels([f"{i+1}. {var}" for i, var in enumerate(final_top10['ë³€ìˆ˜ëª…'])])
    axes[1].set_xlabel('Combined Score', fontsize=11)
    axes[1].set_title('Final Top 10 Variables', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='x')
    
    for i, score in enumerate(final_top10['ì¢…í•©ì ìˆ˜'].values):
        axes[1].text(score, i, f' {score:.3f}', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/6_final_selection.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ“ ì €ì¥: {output_dir}/6_final_selection.png")
    
    print("\nâœ“ 6ë‹¨ê³„ ì™„ë£Œ!")
    print("\n" + "="*100)
    print("ğŸ‰ ë³€ìˆ˜ ì„ íƒ ì™„ë£Œ! ìµœì¢… 10ê°œ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    print("="*100)


# ============================================================================
# 7. ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±
# ============================================================================

def generate_full_report(data_file):
    """ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "="*100)
    print("ğŸ“‹ ì „ì²´ EDA ë¦¬í¬íŠ¸ ìƒì„±")
    print("="*100)
    print("\nëª¨ë“  ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    
    input("\n1ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step1_basic_exploration(data_file)
    
    input("\n2ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step2_missing_outliers(data_file)
    
    input("\n3ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step3_distribution(data_file)
    
    input("\n4ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step4_correlation(data_file)
    
    input("\n5ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step5_feature_importance(data_file)
    
    input("\n6ë‹¨ê³„ ì‹œì‘ - Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    step6_final_selection(data_file)
    
    print("\n" + "="*100)
    print("âœ… ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    print("="*100)
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  - eda_results/3_distribution_analysis.png")
    print("  - eda_results/4_correlation_heatmap.png")
    print("  - eda_results/4_scatter_top9.png")
    print("  - eda_results/5_feature_importance.png")
    print("  - eda_results/6_final_selection.png")
    print("  - eda_results/ìµœì¢…ì„ íƒ_ë³€ìˆ˜10ê°œ.csv")


# ============================================================================
# ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()
